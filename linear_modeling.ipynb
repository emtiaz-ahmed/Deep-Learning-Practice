{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [[1,2], [2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [2, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8a5e45e405b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.Tensor(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "npArray = np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[1 2]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "print(type(npArray))\n",
    "print(npArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor = torch.from_numpy(npArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [2, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = torch.from_numpy(npArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [2, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a9f412f97693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "a = Variable(torch.ones(2,2), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = Variable(torch.ones(2,2), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Variable(torch.ones(2,2), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Variable(torch.Tensor([1,2]), requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 5 * (a +1) ** 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20., 45.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.5000, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = 1/2 * torch.sum(y)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-99366b32e83d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "o.backward()\n",
    "x.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2ac40b2f3c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "o.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.5000, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = 1/2 * torch.sum(y)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a3e9f3974337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-aabbdcdae173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "o.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-91eb991b356e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "o.backward(torch.FloatTensor([1.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3d0b1b5fabd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "o.backward(torch.FloatTensor([1.0, 1.0]), retain_graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1966, 0.1050, 0.0452])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3.], requires_grad = True)\n",
    "out = a.sigmoid()\n",
    "out.sum().backward()\n",
    "a.grad # This will output following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values = [i for i in range(11)]\n",
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_values, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(-1,1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values = [2*i+1 for i in range(11)]\n",
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_values, dtype = np.float32)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.reshape(-1,1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building Model\n",
    "\n",
    "# step 1:\n",
    "   # load library\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 : create model class\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: initiate model class\n",
    "\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: initiate Loss class\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5: set learning rate and create optimizer\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 14.305438041687012\n",
      "epoch 2, loss 1.3376792669296265\n",
      "epoch 3, loss 0.2780323624610901\n",
      "epoch 4, loss 0.18971408903598785\n",
      "epoch 5, loss 0.18064500391483307\n",
      "epoch 6, loss 0.17806075513362885\n",
      "epoch 7, loss 0.1760263741016388\n",
      "epoch 8, loss 0.17405690252780914\n",
      "epoch 9, loss 0.17211274802684784\n",
      "epoch 10, loss 0.1701909750699997\n",
      "epoch 11, loss 0.1682904213666916\n",
      "epoch 12, loss 0.1664111316204071\n",
      "epoch 13, loss 0.16455286741256714\n",
      "epoch 14, loss 0.16271549463272095\n",
      "epoch 15, loss 0.1608983427286148\n",
      "epoch 16, loss 0.1591017097234726\n",
      "epoch 17, loss 0.15732502937316895\n",
      "epoch 18, loss 0.15556807816028595\n",
      "epoch 19, loss 0.1538309007883072\n",
      "epoch 20, loss 0.15211305022239685\n",
      "epoch 21, loss 0.1504145711660385\n",
      "epoch 22, loss 0.14873477816581726\n",
      "epoch 23, loss 0.1470739096403122\n",
      "epoch 24, loss 0.14543156325817108\n",
      "epoch 25, loss 0.14380741119384766\n",
      "epoch 26, loss 0.14220167696475983\n",
      "epoch 27, loss 0.1406138688325882\n",
      "epoch 28, loss 0.13904358446598053\n",
      "epoch 29, loss 0.13749106228351593\n",
      "epoch 30, loss 0.13595551252365112\n",
      "epoch 31, loss 0.13443753123283386\n",
      "epoch 32, loss 0.13293620944023132\n",
      "epoch 33, loss 0.1314515620470047\n",
      "epoch 34, loss 0.12998370826244354\n",
      "epoch 35, loss 0.12853224575519562\n",
      "epoch 36, loss 0.12709692120552063\n",
      "epoch 37, loss 0.12567773461341858\n",
      "epoch 38, loss 0.12427418678998947\n",
      "epoch 39, loss 0.12288648635149002\n",
      "epoch 40, loss 0.1215142011642456\n",
      "epoch 41, loss 0.12015737593173981\n",
      "epoch 42, loss 0.11881554871797562\n",
      "epoch 43, loss 0.11748873442411423\n",
      "epoch 44, loss 0.11617676913738251\n",
      "epoch 45, loss 0.11487948149442673\n",
      "epoch 46, loss 0.11359664052724838\n",
      "epoch 47, loss 0.11232807487249374\n",
      "epoch 48, loss 0.1110738068819046\n",
      "epoch 49, loss 0.10983346402645111\n",
      "epoch 50, loss 0.10860700905323029\n",
      "epoch 51, loss 0.10739409178495407\n",
      "epoch 52, loss 0.10619489848613739\n",
      "epoch 53, loss 0.10500902682542801\n",
      "epoch 54, loss 0.10383643209934235\n",
      "epoch 55, loss 0.10267698764801025\n",
      "epoch 56, loss 0.10153021663427353\n",
      "epoch 57, loss 0.10039645433425903\n",
      "epoch 58, loss 0.0992753729224205\n",
      "epoch 59, loss 0.0981668159365654\n",
      "epoch 60, loss 0.0970705896615982\n",
      "epoch 61, loss 0.09598665684461594\n",
      "epoch 62, loss 0.0949147418141365\n",
      "epoch 63, loss 0.0938548669219017\n",
      "epoch 64, loss 0.09280680865049362\n",
      "epoch 65, loss 0.0917704626917839\n",
      "epoch 66, loss 0.09074565023183823\n",
      "epoch 67, loss 0.08973228186368942\n",
      "epoch 68, loss 0.08873020857572556\n",
      "epoch 69, loss 0.08773946762084961\n",
      "epoch 70, loss 0.0867595449090004\n",
      "epoch 71, loss 0.08579085767269135\n",
      "epoch 72, loss 0.08483284711837769\n",
      "epoch 73, loss 0.08388558775186539\n",
      "epoch 74, loss 0.08294876664876938\n",
      "epoch 75, loss 0.08202255517244339\n",
      "epoch 76, loss 0.08110643178224564\n",
      "epoch 77, loss 0.08020082861185074\n",
      "epoch 78, loss 0.07930540293455124\n",
      "epoch 79, loss 0.07841963320970535\n",
      "epoch 80, loss 0.07754404097795486\n",
      "epoch 81, loss 0.07667802274227142\n",
      "epoch 82, loss 0.07582192867994308\n",
      "epoch 83, loss 0.07497521489858627\n",
      "epoch 84, loss 0.07413773238658905\n",
      "epoch 85, loss 0.07331008464097977\n",
      "epoch 86, loss 0.07249129563570023\n",
      "epoch 87, loss 0.07168187946081161\n",
      "epoch 88, loss 0.07088127732276917\n",
      "epoch 89, loss 0.07008977979421616\n",
      "epoch 90, loss 0.0693071261048317\n",
      "epoch 91, loss 0.06853319704532623\n",
      "epoch 92, loss 0.06776783615350723\n",
      "epoch 93, loss 0.06701111048460007\n",
      "epoch 94, loss 0.06626296043395996\n",
      "epoch 95, loss 0.06552296131849289\n",
      "epoch 96, loss 0.06479126214981079\n",
      "epoch 97, loss 0.06406775861978531\n",
      "epoch 98, loss 0.06335236877202988\n",
      "epoch 99, loss 0.06264492869377136\n",
      "epoch 100, loss 0.06194526329636574\n"
     ]
    }
   ],
   "source": [
    "# step 6: Train model\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    \n",
    "    # convert numpy arrays into torch variable\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    # clear gradients w.r.t parameters\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward to get outputs\n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # calculate loss\n",
    "    \n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # getting gradient w.r.t parameters\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # updating parameters\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"epoch {}, loss {}\".format(epoch, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5370177],\n",
       "       [ 2.6036916],\n",
       "       [ 4.6703653],\n",
       "       [ 6.737039 ],\n",
       "       [ 8.803713 ],\n",
       "       [10.870386 ],\n",
       "       [12.93706  ],\n",
       "       [15.003735 ],\n",
       "       [17.070408 ],\n",
       "       [19.137081 ],\n",
       "       [21.203754 ]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare data\n",
    "\n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.clf() # clear the plot screen\n",
    "\n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "\n",
    "plt.plot(x_train, y_train, 'go', label = 'True', alpha = 0.5)\n",
    "\n",
    "plt.plot(x_train, predicted, '--', label = 'Predictions', alpha = 0.5)\n",
    "\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
